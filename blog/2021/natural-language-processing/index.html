<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> natural language processing | Wonkwon Raymond Lee </title> <meta name="author" content="Wonkwon Raymond Lee"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://wonkwonlee.github.io/blog/2021/natural-language-processing/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Wonkwon</span> Raymond Lee </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">natural language processing</h1> <p class="post-meta"> Created in April 23, 2021 </p> <p class="post-tags"> <a href="/blog/2021"> <i class="fa-solid fa-calendar fa-sm"></i> 2021 </a>   ·   <a href="/blog/tag/study"> <i class="fa-solid fa-hashtag fa-sm"></i> study</a>   <a href="/blog/tag/note"> <i class="fa-solid fa-hashtag fa-sm"></i> note</a>   ·   <a href="/blog/category/bootcamp"> <i class="fa-solid fa-tag fa-sm"></i> bootcamp</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="자연어-처리-natural-language-processing">자연어 처리 Natural Language Processing</h1> <h2 id="자연어-처리-nlp">자연어 처리 NLP</h2> <p><img src="https://user-images.githubusercontent.com/28593767/115187514-6b868180-a11e-11eb-8533-d0914bcb1914.png" alt="nlp"></p> <ul> <li>인간이 일상에서 사용하는 언어를 자연어라고 한다.</li> <li>컴퓨터 분야에서는 자연어 의미를 분석해 컴퓨터가 처리할 수 있도록 하는 일을 <strong>자연어 처리(Natural Language Processing)</strong> 혹은 줄여서 <strong>NLP</strong>라고 한다.</li> <li>일반적으로 문장을 일정한 의미가 있는 <strong>토큰(Token)</strong> 이라는 가장 작은 단어들로 나눈 뒤 나눠진 단어들을 이용해 의미를 분석한다.</li> <li>토큰의 단위는 <strong>토크나이징(Tokenizing)</strong> 방법에 따라 달라질 수 있지만 일반적으로 <em>일정한 의미가 있는 가장 작은 정보 단위로 결정</em>된다.</li> <li> <em>토크나이징은 문장 형태의 데이터를 처리하기 위해 제일 처음 수행해야 하는 기본적인 작업이며 주로 텍스트 전처리 과정에서 사용</em>된다.</li> </ul> <h3 id="konlpy">KoNLPy</h3> <ul> <li> <a href="https://konlpy-ko.readthedocs.io/ko/v0.4.3/" rel="external nofollow noopener" target="_blank"><strong>KoNLPy</strong></a> 는 한국어 토크나이징을 지원하는 파이썬 라이브러리로 한국어 자연어 처리에 많이 사용된다.</li> <li>한국어 문장을 분석하려면 토크나이징 작업을 제일 먼저 수행해야 하는데, 이때 토큰 단위를 어떻게 정의하느냐에 따라 자연어 처리 성능에 영향을 미친다. <ul> <li>일정한 의미가 있는 가장 작은 말의 단위, 즉 의미가 더 이상 쪼개지지 않는 <strong>형태소(Morpheme)</strong> 를 토큰 단위로 사용한다.</li> </ul> </li> <li>영어의 경우 단어의 변화가 크지 않고, 띄어쓰기로 단어를 구분하기 때문에 공백을 기준으로 토크나이징을 수행해도 큰 문제 없지만 <em>한국어는 명사와 조사를 띄어 쓰지 않고, 용언에 따라 여러 가지 어미가 붙기 때문에 띄어쓰기만으로는 토크나이징할 수 없다</em>.</li> <li>따라서 형태소 분석기를 이용하여 문장에서 형태소를 추출하면서 형태소의 뜻과 문맥을 고려해 품사 태깅을 해줘야 한다.</li> </ul> <h3 id="kkma">Kkma</h3> <ul> <li> <a href="http://kkma.snu.ac.kr/documents/?doc=postag" rel="external nofollow noopener" target="_blank"><strong>Kkma</strong></a> 는 서울대학교 IDS(Intelligent Data Systems) 연구실에서 자연어 처리를 위해 개발한 한국형 형태소 분석기로 <em>꼬꼬마</em>라고 발음한다.</li> <li>Kkma는 다음 4가지 함수를 제공한다. <img width="1292" alt="kkma" src="https://user-images.githubusercontent.com/28593767/115185952-cf5b7b00-a11b-11eb-8900-6b10475a3618.png"> </li> </ul> <h3 id="komoran">Komoran</h3> <ul> <li> <a href="https://www.shineware.co.kr/products/komoran/#demo?utm_source=komoran-kr&amp;utm_medium=Referral&amp;utm_campaign=github-demo" rel="external nofollow noopener" target="_blank"><strong>Komoran(Korean Morphological ANalyzer)</strong></a> 은 Shinware에서 개발한 자바 기반 한국어 형태소 분석기로 <em>코모란</em>이라고 발음한다.</li> <li>Komoran은 다음 3가지 함수를 제공한다. <img src="https://user-images.githubusercontent.com/28593767/115186644-0c743d00-a11d-11eb-9d0e-9e10321a5402.png" alt="komoran"> </li> </ul> <h3 id="okt">Okt</h3> <ul> <li> <a href="https://openkoreantext.org/" rel="external nofollow noopener" target="_blank"><strong>Okt(Opensource Korean Text Processor)</strong></a> 는 트위터에서 개발한 Twitter 한국어 처리기에서 파생된 오픈소스 한국어 처리기이다.</li> <li>Okt의 경우 앞서 소개한 형태소 분석기들보다 분석되는 품사 정보는 작지만 분석 속도는 제일 빠르고 또한 normalize() 함수를 지원해 오타가 섞인 문장을 정규화해서 처리하는데 효과적이지만 성능이 뛰어나지는 않다.</li> <li>Okt는 다음 5가지 함수를 제공한다. <img src="https://user-images.githubusercontent.com/28593767/115187507-69bcbe00-a11e-11eb-9740-bb14fa6aa3ec.png" alt="okt"> </li> </ul> <h3 id="한국어-토크나이징">한국어 토크나이징</h3> <ul> <li>영어의 경우 단순히 토큰 정보만 필요하다면 띄어쓰기만 하더라도 훌륭한 결과를 보여준다.</li> <li>하지만 <em>한국어는 명사와 조사를 띄어쓰지 않고, 용언에 따라 여러 가지 어미가 붙기 때문에 띄어쓰기만으로는 토크나이징을 할 수 없다</em>.</li> <li>따라서 KoNLPy의 형태소 분석기를 이용해 형태소 단위의 토큰과 품사 정보까지 추출하고 추출된 정보에서 필요 없는 정보를 제거하는 <strong>전처리(Preprocessing)</strong> 과정이 추가되어야 한다.</li> </ul> <p><img src="https://user-images.githubusercontent.com/28593767/115188134-71c92d80-a11f-11eb-9dbc-e3db0889475e.png" alt="konlpy"></p> <h2 id="임베딩-embedding">임베딩 Embedding</h2> <ul> <li>컴퓨터는 수치 연산만 가능하기 때문에 자연어를 숫자나 벡터 형태로 변환해야 한다.</li> <li>임베딩은 <strong>단어나 문장을 수치화해 벡터 공간으로 표현하는 과정</strong>을 의미한다.</li> <li>임베딩은 말뭉치의 의미에 따라 벡터화하기 때문에 문법적인 정보가 포함되어 있다.</li> <li>임베딩 기법에는 문장 전체를 벡터로 표현하는 문장 임베딩과 개별 단어를 벡터로 표현하는 단어 임베딩이 있다. <ul> <li>문장 임베딩의 경우 <em>전체 문장의 흐름을 파악해 벡터로 변환하기 때문에 문맥적 의미를 지니는 장점</em>이 있다.</li> <li>하지만 <em>임베딩하기 위해 많은 문장 데이터가 필요하며 학습하는데 비용</em>이 많이 들어간다.</li> <li>반면 단어 임베딩은 문장 임베딩에 비해 학습 방법이 간단해 <em>실무에서 많이 사용</em>된다.</li> <li>하지만 단어 임베딩은 <em>동음이의어에 대한 구분을 하지 않기 때문에 의미가 다르더라도 단어의 형태가 같다면 동일한 벡터값으로 표현되는 단점</em>이 있다.</li> </ul> </li> <li>단어 임베딩은 말뭉치에서 각각의 단어를 벡터로 변환하는 기법으로 의미와 문법적 정보를 지니고 있으며, 단어를 표현하는 방법에 따라 다양한 모델이 존재한다.</li> </ul> <h3 id="원-핫-인코딩-one-hot-encoding">원-핫 인코딩 One-Hot Encoding</h3> <p><img width="484" alt="onehot" src="https://user-images.githubusercontent.com/28593767/115192638-09317f00-a126-11eb-8c21-4569f91c4aa4.png"></p> <ul> <li>원-핫 인코딩은 단어를 숫자 벡터로 변환하는 가장 기본적인 방법으로 <em>단 하나의 값만 1이고 나머지 요솟값은 0인 인코딩</em>을 의미한다.</li> <li>원-핫 인코딩으로 나온 결과를 <strong>원-핫 벡터(One-Hot Vector)</strong> 라 하며, 전체 요소 중 단 하나의 값만 1이기 때문에 <strong>희소 벡터(Sparse Vector)</strong> 라고 한다.</li> <li>원-핫 인코딩을 하기 위해서는 단어 집합이라 불리는 사전을 먼저 만들어야 한다. <ul> <li>사전이란 말뭉치에서 나오는 서로 다른 모든 단어의 집합을 의미한다.</li> <li>말뭉치에 존재하는 모든 단어의 수가 원-핫 벡터의 차원을 결정한다.</li> <li>사전 내 단어 순서대로 고유한 인덱스 번호를 부여해 단어의 인덱스 번호가 원-핫 인코딩에서 1의 값을 가지는 요소의 위치가 부여된다.</li> </ul> </li> <li>이처럼 단어가 희소 벡터로 표현되는 방식을 <strong>희소 표현(Sparse Representation)</strong> 이라고 한다. <ul> <li>희소 표현은 각각의 차원이 독립적인 정보를 지니고 있어 사람이 이해하기에 직관적인 장점이 있지만 <em>단어 사전의 크기가 커질수록 메모리 낭비와 계산 복잡도가 커지는 단점</em>이 있다.</li> <li>또한 <em>기본 토큰이 되는 단어의 의미와 주변 단어 간의 관계를 단어 임베딩에 표현할 수 없다는 단점</em>도 있다.</li> </ul> </li> </ul> <h3 id="분산-표현-distributed-representation">분산 표현 Distributed Representation</h3> <p><img width="394" alt="distri" src="https://user-images.githubusercontent.com/28593767/115192633-0767bb80-a126-11eb-9314-086c7f80e740.png"></p> <ul> <li>분산 표현은 한 단어의 정보가 특정 차원에 표현되지 않고 여러 차원에 분산되어 표현되어 붙여진 이름이다. <ul> <li>원하는 차원에 데이터를 최대한 밀집시킬 수 있어 <strong>밀집 표현(Dense Representation)</strong> 이라 부르기도 하며 밀집 표현으로 만들어 진 벡터를 <strong>밀집 벡터(Dense Vector)</strong> 라고 한다.</li> </ul> </li> <li>희소 표현과 달리 <em>각 단어 간의 유사성을 잘 표현하면서도 벡터 공간을 절약할 수 있는 방법</em>으로 하나의 차원에 다양한 정보를 가지고 있다.</li> <li>분산 표현의 첫번째 장점은 <strong>임베딩 벡터의 차원을 데이터 손실을 최소화하면서 압축할 수 있다</strong>는 점이다. <ul> <li>희소 표현 방식은 단어를 표현하는 데 너무 많은 차원이 필요하고 단어 사전이 커질수록 비효율적이고 희소 벡터이기 때문에 대부분의 값이 0이 된다.</li> <li>입력 데이터의 차원이 너무 높아지면 신경망 모델의 학습이 어려워지는 <strong>차원의 저주(Curse of Dimentionality)</strong> 문제가 발생한다.</li> </ul> </li> <li>또한 분산 표현의 두 번째 장점은 <strong>임베딩 벡터에는 단어의 의미, 주변 단어간의 관계 등 많은 정보가 표현되어 있어 일반화 능력이 뛰어나다</strong>는 점이다. <ul> <li>예를 들어 남자와 남성이라는 단어가 있을 때 희소 표현 방식에는 그저 단 하나의 요솟값에 불과하다.</li> <li>즉, 남자와 남성의 관계가 전혀 표현되어 있지 않는다.</li> <li>하지만 분산 표현에서는 <em>벡터 공간 상에서 유사한 의미를 갖는 단어들은 비슷한 위치에 분포</em>되어 있기 때문에 남자와 남성의 단어 위치는 매우 가깝다.</li> </ul> </li> </ul> <h3 id="word2vec-모델">Word2Vec 모델</h3> <ul> <li>Word2Vec 모델은 신경망 기반 단어 임베딩의 대표적인 방법으로 2013년에 구글에서 발표했으며 가장 많이 사용되는 단어 임베딩 모델이다.</li> <li>기존 신경망 기반의 단어 임베딩 모델에 비해 구조상 차이는 크게 없지만 <em>계산량을 획기적으로 줄여 빠른 학습을 가능</em>하게 하였다.</li> <li>Word2Vec 모델은 <strong>CBOW(Continuous Bag of Words)</strong> 와 <strong>skip-gram</strong> 두 가지 모델로 제안되었다.</li> </ul> <p><img width="891" alt="word2vec" src="https://user-images.githubusercontent.com/28593767/115327256-8f52d180-a1c9-11eb-9bb9-1497384c4c0f.png"></p> <ul> <li>CBOW 모델은 <strong>맥락(Context Word)이라 표현되는 주변 단어들을 이용해 타겟 단어를 예측</strong>하는 신경망 모델이다. <ul> <li>신경망의 입력을 주변 단어들로 구성하고 출력을 타깃 단어로 설정해 학습된 가중치 데이터를 임베딩 벡터로 활용한다.</li> <li>CBOW 모델은 <em>타깃 단어의 손실만 계산하면 되기 때문에 학습 속도가 빠른 장점</em>이 있다.</li> </ul> </li> <li>skip-gram 모델은 CBOW 모델과 반대로 <strong>하나의 타깃 단어를 이용해 주변 단어들을 예측</strong>하는 신경망 모델이다. <ul> <li>skip-gram 모델은 입출력이 CBOW 모델과 반대로 되어 있기 때문에 예측해야 하는 맥락이 많다.</li> <li>따라서 <em>단어 분산 표현력이 우수해 CBOW 모델에 비해 임베딩 품질이 우수</em>하다.</li> </ul> </li> <li>CBOW 모델에서는 타깃 단어를 예측하기 위해 앞뒤 단어를 확인하는데 이 때 <em>앞뒤로 몇 개의 단어까지 확인할지 결정하는 범위</em>를 <strong>윈도우(Window)</strong> 라고 한다.</li> </ul> <p><img width="915" alt="window_size" src="https://user-images.githubusercontent.com/28593767/115327554-1f911680-a1ca-11eb-8c6b-37b7d5dbe4fd.png"></p> <ul> <li>Word2Vec의 단어 임베딩은 해당 단어를 밀집 벡터로 표현하며 학습을 통해 의미상 비슷한 단어들을 비슷한 벡터 공간에 위치시킨다.</li> <li>벡터 특성상 의미에 따라 방향성을 지니고 임베딩된 벡터들 간 연산이 가능하기 때문에 단어간 관계를 계산할 수 있다. <ul> <li>왕과 여왕의 방향 차이만큼 남자와 여자의 방향 차이가 생긴다.</li> </ul> </li> </ul> <p><img width="398" alt="wordvec" src="https://user-images.githubusercontent.com/28593767/115328362-7a773d80-a1cb-11eb-9c44-0864b2dacba5.png"></p> <h2 id="text-similarity-algorithm">Text Similarity Algorithm</h2> <ul> <li>인간은 두 개의 문장에 동일한 단어나 의미상 비슷한 단어들이 얼마나 분포되어 있는지 직감적으로 파악한다.</li> <li>컴퓨터는 <em>임베딩으로 각 단어들의 벡터를 구한 다음 벡터 간의 거리를 계산하는 방법으로 단어 간의 의미가 얼마나 유사 한지 계산</em>할 수 있다.</li> <li>Q&amp;A 챗봇 개발을 위해서는 챗봇 엔진에 입력되는 문장과 시스템에서 해당 주제의 답변과 연관되어 있는 질문이 얼마나 유사 한지 계산할 수 있어야 적절한 답변을 출력할 수 있다.</li> <li>두 문장 간의 유사도를 계산하기 위해서는 문장 내에 존재하는 단어들을 수치화해야 하는데 이 때 언어 모델에 따라 통계를 이용하는 방법과 인공 신경망을 이용하는 방법으로 나눌 수 있다. <ul> <li>Word2Vec은 인공 신경망을 이용한 방법이고 이제 통계적인 방법을 이용해 유사도를 계산하는 방법을 살펴본다.</li> </ul> </li> </ul> <h3 id="n-gram">n-gram</h3> <p><img src="https://user-images.githubusercontent.com/28593767/115339998-e021f480-a1e0-11eb-8ca5-a8b3303bdaca.png" alt="n-gram"></p> <ul> <li>n-gram은 주어진 문장에서 n개의 연속적인 단어 시퀀스(단어 나열)를 의미한다.</li> <li>n-gram은 문장에서 n개의 단어를 토큰으로 사용하여 <em>이웃한 단어의 출현 횟수를 통계적으로 표현해 텍스트의 유사도를 계산하는 방법</em>이다.</li> <li> <em>서로 다른 문장을 n-gram으로 비교하면 단어의 출현 빈도에 기반한 유사도를 계산</em>할 수 있고 이를 통해 논문 인용이나 도용 정도를 조사할 수 있다. <ul> <li>구현하기는 쉽지만 학습 말뭉치 품질만 좋다면 괜찮은 성능을 보여준다.</li> </ul> </li> </ul> <p><img src="https://user-images.githubusercontent.com/28593767/115339999-e1532180-a1e0-11eb-97cb-0a0c1e3e09bf.png" alt="similarity"></p> <ul> <li> <p>n-gram을 이용한 문장 간의 유사도를 계산</p> <ol> <li>문장을 n-gram으로 토큰을 분리한 후 <strong>단어 문서 행렬(Term-Document Matrix, DTM)</strong> 을 만든다.</li> <li>두 문장을 서로 비교해 동일한 단어의 출현 빈도를 확률로 계산해 유사도를 구한다.</li> </ol> </li> <li> <strong>tf(Term Frequency)</strong> 는 두 문장 A와 B에서 동일한 토큰의 출현 빈도를 뜻하며 tokens는 해당 문장에서 전체 토큰 수를 의미한다. <ul> <li>여기서 토큰은 n-gram으로 분리된 단어를 뜻한다.</li> <li>즉, 위의 Similarity 수식은 <em>기준이 되는 문장 A의 전체 토큰 중 A와 B에 동일한 토큰이 얼마나 있는지를 비율로 나타낸 수식</em>이다.</li> </ul> </li> <li>실무에서는 문서 단위로 유사도를 구하는 경우가 많고 해당 문서에서 단어들이 얼마나 나오는지 출현 빈도를 행렬로 표현한다.</li> </ul> <p><img src="https://user-images.githubusercontent.com/28593767/115342311-06e22a00-a1e5-11eb-8459-5eaa0fc94b68.png" alt="n-gram"></p> <p>문장 A와 B의 유사도는 4/6, 0.66의 유사도를 가지고 다시 말해 <em>문장 A와 B는 66% 유사하다.</em></p> <h3 id="코사인-유사도-cosine-similarity">코사인 유사도 Cosine Similarity</h3> <p><img width="568" alt="cos" src="https://user-images.githubusercontent.com/28593767/115342307-05b0fd00-a1e5-11eb-8a61-70021a8889f0.png"></p> <ul> <li>단어나 문장을 벡터로 표현할 때 벡터 간 거리나 각도를 이용해 유사성을 파악할 수 있다.</li> <li>코사인 유사도는 <em>두 벡터간 코사인 각도를 이용해 유사도를 측정하는 방법으로 일반적으로 벡터의 크기가 중요하지 않을 때 그 거리를 측정하기 위해 사용</em>한다. <ul> <li>단어의 출현 빈도를 통해 유사도 계산을 한다면 동일한 단어가 많이 포함되어 있을수록 벡터의 크기가 커지지만 <em>코사인 유사도는 벡터의 크기와 상관없이 결과가 안정적</em>이다.</li> <li>코사인 값은 -1 ~ 1 사이의 값을 가지며 두 벡터의 방향이 완전히 동일한 경우에는 1, 반대 방향인 경우에는 -1, 두 벡터가 서로 직각을 이루면 0의 값을 가진다.</li> <li>즉, 두 벡터의 방향이 같아질 수록 유사한 값을 지닌다.</li> </ul> </li> </ul> <p><img src="https://user-images.githubusercontent.com/28593767/115342313-077ac080-a1e5-11eb-9a69-6ee58db51aae.png" alt="cos2"></p> <p>문장 A와 B의 코사인 각도는 0.83이고 다시 말해 <em>문장 A와 B는 83% 유사하다.</em></p> <blockquote> <p>자연어 처리를 위해서는 제일 먼저 <strong>토크 나이징을 통해 주어진 문장에서 최소한의 의미를 가지는 단어들을 토큰화</strong> 시켜야 한다. 추출한 토큰들은 아직 자연어 형태이므로 컴퓨터가 연산하거나 처리할 수 없기 때문에 <strong>임베딩 처리를 통해 컴퓨터가 계산하기 용이한 벡터 형태로 수치화</strong> 시키는 과정을 거쳐야 한다. 임베딩된 토큰들은 <strong>출현 빈도를 이용하거나 인공 신경망을 통해 문장 간의 유사도를 계산</strong>할 수 있다.</p> <p><em>챗봇 엔진에 어떤 질문이 입력되었을 때 적절한 답변을 하기 위해서는 입력된 질문과 시스템에 저장되어 있는 질문–답변 데이터의 유사도를 계산할 수 있어야 해당 질문에 연관된 답변을 할 수 있다.</em></p> </blockquote> <h2 id="deep-learning-models">Deep Learning Models</h2> <ul> <li>챗봇 엔진에서 <strong>문장 의도 분류</strong>를 위해서는 <strong>CNN(Convolutional Neural Network) 모델</strong>을 사용한다.</li> <li>일반적으로 이미지 분류에서 좋은 성능을 보이지만 <em>임베딩 품질만 괜찮다면 자연어 분류에도 좋은 성능</em>을 보여준다. <ul> <li>컴퓨터 입장에서는 이미지든 임베딩 처리된 자연어든 <em>수치(벡터)로 표현 가능한 대상이면 특징을 뽑아내도록 CNN 모델을 학습</em>할 수 있다.</li> </ul> </li> </ul> <p><img src="https://user-images.githubusercontent.com/28593767/115499945-5d19a080-a2ab-11eb-98d1-468b23b32234.png" alt="cnn_diagram"></p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/mapping-the-mind-of-a-large-language-model-anthropic/">Mapping the Mind of a Large Language Model \ Anthropic</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/generative-adversarial-network/">generative adversarial network</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/reinforcement-learning/">reinforcement learning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/database/">database</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/recurrent-neural-network/">recurrent neural network</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Wonkwon Raymond Lee. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"publications",description:"publications in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"A collection of my projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-repositories",title:"repositories",description:"A brief showcase of my repositories.",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"cv",description:"Professional Curriculum Vitae",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"dropdown-publications",title:"publications",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-projects",title:"projects",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-blog",title:"blog",description:"",section:"Dropdown",handler:()=>{window.location.href="/blog/"}},{id:"post-mapping-the-mind-of-a-large-language-model-anthropic",title:'Mapping the Mind of a Large Language Model  Anthropic <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"We have identified how millions of concepts are represented inside Claude Sonnet, one of our deployed large language models. This is the first ever detailed look inside a modern, production-grade large language model.",section:"Posts",handler:()=>{window.open("https://www.anthropic.com/research/mapping-mind-language-model","_blank")}},{id:"post-generative-adversarial-network",title:"generative adversarial network",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/generative-adversarial-network/"}},{id:"post-reinforcement-learning",title:"reinforcement learning",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/reinforcement-learning/"}},{id:"post-database",title:"database",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/database/"}},{id:"post-natural-language-processing",title:"natural language processing",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/natural-language-processing/"}},{id:"post-recurrent-neural-network",title:"recurrent neural network",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/recurrent-neural-network/"}},{id:"post-convolutional-neural-network",title:"convolutional neural network",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/convolutional-neural-network/"}},{id:"post-linear-algebra",title:"linear algebra",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/linear-algebra/"}},{id:"post-adaptive-moments-estimation",title:"adaptive moments estimation",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/adaptive-moments-estimation/"}},{id:"post-shortest-path-algorithm",title:"shortest path algorithm",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/shortest-path-algorithm/"}},{id:"post-dynamic-programming",title:"dynamic programming",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/dynamic-programming/"}},{id:"post-search-algorithm",title:"search algorithm",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/search-algorithm/"}},{id:"post-multi-layer-perceptron",title:"multi layer perceptron",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/multi-layer-perceptron/"}},{id:"post-classification",title:"classification",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/classification/"}},{id:"post-tree",title:"tree",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/tree/"}},{id:"post-sorting-algorithm",title:"sorting algorithm",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/sorting-algorithm/"}},{id:"post-regression-analysis",title:"regression analysis",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/regression-analysis/"}},{id:"post-stack-and-queue",title:"stack and queue",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/stack-and-queue/"}},{id:"post-jupyter-and-markdown",title:"jupyter and markdown",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/jupyter-and-markdown/"}},{id:"post-dfs-and-bfs",title:"dfs and bfs",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/dfs-and-bfs/"}},{id:"post-single-layer-perceptron",title:"single layer perceptron",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/single-layer-perceptron/"}},{id:"post-linear-regression",title:"linear regression",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/linear-regression/"}},{id:"post-foundations-of-mathematics",title:"foundations of mathematics",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/foundations-of-mathematics/"}},{id:"post-foundations-of-artificial-intelligence",title:"foundations of artificial intelligence",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/foundations-of-artificial-intelligence/"}},{id:"post-differential",title:"differential",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/differential/"}},{id:"post-complexity",title:"complexity",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/complexity/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-out-of-distribution-robustness-evaluation-of-state-of-the-art-vision-models",title:"Out-of-distribution Robustness Evaluation of State-of-the-art Vision Models",description:"Research project comparing the robustness of 58 computer vision models",section:"Projects",handler:()=>{window.location.href="/projects/1_ood/"}},{id:"projects-time-series-medical-image-classification",title:"Time-series Medical Image Classification",description:"Image Classification Model for Temporal Disease Progression of Chest X-ray dataset",section:"Projects",handler:()=>{window.location.href="/projects/2_timeseries_medical/"}},{id:"projects-epistemic-parity-reproducibility-as-an-evaluation-metric-for-differential-privacy",title:"Epistemic Parity: Reproducibility as an Evaluation Metric for Differential Privacy",description:"A benchmark and evaluation for reproducibility in differential privacy with state-of-the-art DP synthesizers.",section:"Projects",handler:()=>{window.location.href="/projects/3_synrd/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%77%6F%6E%6B%77%6F%6E.%6C%65%65@%6E%79%75.%65%64%75","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=xXZLYFwAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/wonkwonlee","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/wonkwon-lee","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>