<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Out-of-distribution Robustness Evaluation of State-of-the-art Vision Models | Wonkwon Raymond Lee </title> <meta name="author" content="Wonkwon Raymond Lee"> <meta name="description" content="Research project comparing the robustness of 58 computer vision models"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://wonkwonlee.github.io/projects/1_ood/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Wonkwon</span> Raymond Lee </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Out-of-distribution Robustness Evaluation of State-of-the-art Vision Models</h1> <p class="post-description">Research project comparing the robustness of 58 computer vision models</p> </header> <article> <p>This project provides a thorough out-of-distribution (OOD) robustness comparison across 58 state-of-the-art computer vision models. These include models based on vision transformers, convolutional networks, hybrid mechanisms, and more.</p> <p>The research investigates how different architectures perform under various distribution shifts, highlighting the strengths and weaknesses of each approach in terms of robustness.</p> <p>The full paper can be accessed on <a href="https://doi.org/10.48550/arXiv.2301.10750" rel="external nofollow noopener" target="_blank">arXiv</a> and the code is available on <a href="https://github.com/salman-lui/vision_course_project" rel="external nofollow noopener" target="_blank">GitHub</a>.</p> <h2 id="summary">Summary</h2> <p>This work offers a comparative study of out-of-distribution robustness across 58 models, such as vision transformers, CNNs, hybrid models, and others. The analysis was conducted using a unified training setup, covering diverse models like convolution-based, attention-based, sequence-based, and more. The findings reveal how robustness varies depending on model type and the OOD type being evaluated.</p> <h2 id="abstract">Abstract</h2> <p>Vision transformers (ViT) have emerged as cutting-edge tools in visual recognition tasks, often surpassing CNNs in robustness due to their self-attention mechanisms. However, previous studies may have been biased by unfair experimental setups, only comparing a small number of models.</p> <p>Our study explores 58 models, comparing attention, convolution, hybrid, and sequence-based architectures under fair and controlled conditions. Our findings demonstrate that robustness is not solely determined by architecture but also by training setup and OOD data type.</p> <p>The research aims to assist the community in better understanding the robustness of vision models.</p> <h2 id="methods">Methods</h2> <p>We evaluated these models using benchmark OOD datasets, such as ImageNet-A, ImageNet-R, ImageNet-Sketch, and others. We trained and tested models using standardized setups to ensure fair comparisons.</p> <ul> <li> <strong>Convolution-based models</strong>: ResNet-50, DenseNet, and EfficientNet</li> <li> <strong>Attention-based models</strong>: ViT, DeiT, BEiT</li> <li> <strong>Hybrid models</strong>: ConViT, ConvNext</li> <li> <strong>Other models</strong>: MLP-Mixer, RegNet, Sequencer</li> </ul> <h3 id="top-5-convolution-based-models">Top 5 Convolution-Based Models</h3> <table> <thead> <tr> <th><strong>Architecture</strong></th> <th><strong>ImageNet</strong></th> <th><strong>ImageNet -A</strong></th> <th><strong>ImageNet -O</strong></th> <th><strong>ImageNet -R</strong></th> <th><strong>Stylized-ImageNet</strong></th> <th><strong>ImageNet -Sketch</strong></th> </tr> </thead> <tbody> <tr> <td>ConvNext</td> <td>87.8</td> <td>35.79</td> <td>2.18</td> <td>51.71</td> <td>19.24</td> <td>38.22</td> </tr> <tr> <td>EfficientNetV2</td> <td>87.3</td> <td>22.31</td> <td>2.21</td> <td>52.97</td> <td>23.3</td> <td>40.33</td> </tr> <tr> <td>ResNeSt</td> <td>83.9</td> <td>5.47</td> <td>2.2</td> <td>37.77</td> <td>7.39</td> <td>25.28</td> </tr> <tr> <td>ResNet</td> <td>79.29</td> <td>1.88</td> <td>2.23</td> <td>28.93</td> <td>3.59</td> <td>16.84</td> </tr> <tr> <td>Xception</td> <td>79.88</td> <td>3.53</td> <td>2.28</td> <td>33.52</td> <td>9.33</td> <td>18.26</td> </tr> </tbody> </table> <h4 id="convolution-based-models">Convolution-Based Models</h4> <p>Convolution-based models, such as ConvNets, process visual data through layers of convolutions, pooling, and non-linear activations. They excel at capturing local spatial hierarchies in images, making them highly effective for tasks like object recognition and classification. Over time, innovations like depthwise separable convolutions and residual connections have increased their efficiency and depth, leading to architectures like <strong>ConvNext</strong> and <strong>EfficientNetV2</strong>.</p> <h3 id="top-5-attention-based-models">Top 5 Attention-Based Models</h3> <table> <thead> <tr> <th><strong>Architecture</strong></th> <th><strong>ImageNet</strong></th> <th><strong>ImageNet -A</strong></th> <th><strong>ImageNet -O</strong></th> <th><strong>ImageNet -R</strong></th> <th><strong>Stylized-ImageNet</strong></th> <th><strong>ImageNet -Sketch</strong></th> </tr> </thead> <tbody> <tr> <td>CaiT</td> <td>86.5</td> <td>34.4</td> <td>2.2</td> <td>49.95</td> <td>19.48</td> <td>36.03</td> </tr> <tr> <td>CrossViT</td> <td>81.5</td> <td>31.8</td> <td>2.2</td> <td>47.1</td> <td>18.51</td> <td>33.49</td> </tr> <tr> <td>DeiT3</td> <td>83.1</td> <td>40.12</td> <td>2.22</td> <td>53.63</td> <td>25.45</td> <td>40.06</td> </tr> <tr> <td>Swin</td> <td>87.3</td> <td>39.41</td> <td>2.19</td> <td>48.64</td> <td>18.36</td> <td>34.37</td> </tr> <tr> <td>VOLO</td> <td>87.1</td> <td>41.03</td> <td>2.23</td> <td>48.53</td> <td>17.74</td> <td>36.11</td> </tr> </tbody> </table> <h4 id="attention-based-models">Attention-Based Models</h4> <p>Attention-based models, like Vision Transformers (ViT), capture long-range dependencies within images by applying self-attention mechanisms. Instead of relying on convolutional operations, these models divide images into patches and use multi-head attention to learn relationships between different parts of the image. Attention models excel at capturing global context, achieving state-of-the-art results in tasks requiring holistic image understanding, as seen in models like <strong>Swin</strong> and <strong>DeiT3</strong>.</p> <h3 id="top-5-hybrid-convolution--attention-models">Top 5 Hybrid (Convolution + Attention) Models</h3> <table> <thead> <tr> <th><strong>Architecture</strong></th> <th><strong>ImageNet</strong></th> <th><strong>ImageNet -A</strong></th> <th><strong>ImageNet -O</strong></th> <th><strong>ImageNet -R</strong></th> <th><strong>Stylized-ImageNet</strong></th> <th><strong>ImageNet -Sketch</strong></th> </tr> </thead> <tbody> <tr> <td>ConViT</td> <td>81.3</td> <td>29.69</td> <td>2.21</td> <td>48.79</td> <td>19.81</td> <td>35.53</td> </tr> <tr> <td>EdgeNeXt</td> <td>71.2</td> <td>21.83</td> <td>2.23</td> <td>51.04</td> <td>16.26</td> <td>37.13</td> </tr> <tr> <td>LeViT</td> <td>80.0</td> <td>11.51</td> <td>2.22</td> <td>40.28</td> <td>15.59</td> <td>26.39</td> </tr> <tr> <td>RegNet</td> <td>80.25</td> <td>26.48</td> <td>2.17</td> <td>44.74</td> <td>11.88</td> <td>32.93</td> </tr> <tr> <td>Sequencer</td> <td>84.6</td> <td>35.36</td> <td>2.18</td> <td>48.56</td> <td>16.74</td> <td>35.87</td> </tr> </tbody> </table> <h4 id="hybrid-models-convolution--attention">Hybrid Models (Convolution + Attention)</h4> <p>Hybrid models combine the strengths of both convolution and attention mechanisms. These architectures use convolution layers to extract low-level features and attention mechanisms to capture global dependencies. By fusing these two approaches, hybrid models like <strong>ConViT</strong> and <strong>LeViT</strong> balance local feature extraction with global context understanding, achieving both efficiency and accuracy in visual tasks.</p> <h3 id="other-architectures">Other Architectures</h3> <table> <thead> <tr> <th><strong>Architecture</strong></th> <th><strong>ImageNet</strong></th> <th><strong>ImageNet -A</strong></th> <th><strong>ImageNet -O</strong></th> <th><strong>ImageNet -R</strong></th> <th><strong>Stylized-ImageNet</strong></th> <th><strong>ImageNet -Sketch</strong></th> </tr> </thead> <tbody> <tr> <td>MLPMixer</td> <td>76.44</td> <td>5.79</td> <td>2.25</td> <td>32.75</td> <td>10.04</td> <td>19.32</td> </tr> <tr> <td>MobileNetV3</td> <td>75.77</td> <td>4.59</td> <td>2.2</td> <td>35.11</td> <td>10.85</td> <td>22.95</td> </tr> <tr> <td>RegNet</td> <td>80.25</td> <td>26.48</td> <td>2.17</td> <td>44.74</td> <td>11.88</td> <td>32.93</td> </tr> <tr> <td>Sequencer</td> <td>84.6</td> <td>35.36</td> <td>2.18</td> <td>48.56</td> <td>16.74</td> <td>35.87</td> </tr> </tbody> </table> <h4 id="explanation-of-the-top-5-other-architectures">Explanation of the Top 5 Other Architectures</h4> <ol> <li> <p><strong>MLPMixer</strong>:</p> <ul> <li>Uses MLPs instead of convolutions or self-attention for vision tasks by mixing spatial and channel information. Efficient for large datasets.</li> </ul> </li> <li> <p><strong>MobileNetV3</strong>:</p> <ul> <li>Lightweight model designed for mobile devices, using depthwise separable convolutions and neural architecture search (NAS) to optimize performance with low computational cost.</li> </ul> </li> <li> <p><strong>RegNet</strong>:</p> <ul> <li>Scalable model family balancing depth and width, designed for efficient computation across different resource constraints, performing well on large vision tasks.</li> </ul> </li> <li> <p><strong>Sequencer</strong>:</p> <ul> <li>A vision model that adapts sequence modeling techniques from NLP, processing images as sequences to capture long-range dependencies.</li> </ul> </li> </ol> <p><span id="rahman2023out">Rahman, S., &amp; Lee, W. (2023). Out of distribution performance of state of art vision model. <i>ArXiv Preprint ArXiv:2301.10750</i>.</span></p> </article> <h2>References</h2> <div class="publications"> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Wonkwon Raymond Lee. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"A collection of my projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-repositories",title:"repositories",description:"A brief showcase of my repositories.",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"cv",description:"Professional Curriculum Vitae",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"dropdown-publications",title:"publications",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-projects",title:"projects",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-blog",title:"blog",description:"",section:"Dropdown",handler:()=>{window.location.href="/blog/"}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/tabs/"}},{id:"post-a-post-with-typograms",title:"a post with typograms",description:"this is what included typograms code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/typograms/"}},{id:"post-a-post-that-can-be-cited",title:"a post that can be cited",description:"this is what a post that can be cited looks like",section:"Posts",handler:()=>{window.location.href="/blog/2024/post-citation/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/pseudocode/"}},{id:"post-a-post-with-code-diff",title:"a post with code diff",description:"this is how you can display code diffs",section:"Posts",handler:()=>{window.location.href="/blog/2024/code-diff/"}},{id:"post-a-post-with-advanced-image-components",title:"a post with advanced image components",description:"this is what advanced image components could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/advanced-images/"}},{id:"post-a-post-with-vega-lite",title:"a post with vega lite",description:"this is what included vega lite code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/vega-lite/"}},{id:"post-a-post-with-geojson",title:"a post with geojson",description:"this is what included geojson code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/geojson-map/"}},{id:"post-a-post-with-echarts",title:"a post with echarts",description:"this is what included echarts code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/echarts/"}},{id:"post-a-post-with-chart-js",title:"a post with chart.js",description:"this is what included chart.js code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/chartjs/"}},{id:"post-a-post-with-tikzjax",title:"a post with TikZJax",description:"this is what included TikZ code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/tikzjax/"}},{id:"post-a-post-with-bibliography",title:"a post with bibliography",description:"an example of a blog post with bibliography",section:"Posts",handler:()=>{window.location.href="/blog/2023/post-bibliography/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/blog/2023/jupyter-notebook/"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/blog/2023/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/blog/2023/sidebar-table-of-contents/"}},{id:"post-a-post-with-audios",title:"a post with audios",description:"this is what included audios could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/audios/"}},{id:"post-a-post-with-videos",title:"a post with videos",description:"this is what included videos could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/videos/"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/blog/2023/tables/"}},{id:"post-a-post-with-table-of-contents",title:"a post with table of contents",description:"an example of a blog post with table of contents",section:"Posts",handler:()=>{window.location.href="/blog/2023/table-of-contents/"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/blog/2022/giscus-comments/"}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/assets/pdf/example_pdf.pdf"}},{id:"post-a-post-with-diagrams",title:"a post with diagrams",description:"an example of a blog post with diagrams",section:"Posts",handler:()=>{window.location.href="/blog/2021/diagrams/"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/blog/2021/distill/"}},{id:"post-a-post-with-twitter",title:"a post with twitter",description:"an example of a blog post with twitter",section:"Posts",handler:()=>{window.location.href="/blog/2020/twitter/"}},{id:"post-a-post-with-disqus-comments",title:"a post with disqus comments",description:"an example of a blog post with disqus comments",section:"Posts",handler:()=>{window.location.href="/blog/2015/disqus-comments/"}},{id:"post-a-post-with-math",title:"a post with math",description:"an example of a blog post with some math",section:"Posts",handler:()=>{window.location.href="/blog/2015/math/"}},{id:"post-a-post-with-code",title:"a post with code",description:"an example of a blog post with some code",section:"Posts",handler:()=>{window.location.href="/blog/2015/code/"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2015/images/"}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march & april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-out-of-distribution-robustness-evaluation-of-state-of-the-art-vision-models",title:"Out-of-distribution Robustness Evaluation of State-of-the-art Vision Models",description:"Research project comparing the robustness of 58 computer vision models",section:"Projects",handler:()=>{window.location.href="/projects/1_ood/"}},{id:"projects-time-series-medical-image-classification",title:"Time-series Medical Image Classification",description:"Image Classification Model for Temporal Disease Progression of Chest X-ray dataset",section:"Projects",handler:()=>{window.location.href="/projects/2_timeseries_medical/"}},{id:"projects-differentially-private-synthetic-data-research",title:"Differentially Private Synthetic Data Research",description:"Synthetic Dataset Feasiblity Study",section:"Projects",handler:()=>{window.location.href="/projects/3_synrd/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%77%6F%6E%6B%77%6F%6E.%6C%65%65@%6E%79%75.%65%64%75","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=xXZLYFwAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/wonkwonlee","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/wonkwon-lee","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>