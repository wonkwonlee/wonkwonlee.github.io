<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content=""> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Out-of-distribution Robustness Evaluation of State-of-the-art Vision Models | Wonkwon Raymond Lee </title> <meta name="author" content="Wonkwon Raymond Lee"> <meta name="description" content="Research project comparing the robustness of 58 computer vision models"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://wonkwonlee.github.io/projects/1_ood/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Wonkwon</span> Raymond Lee </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Out-of-distribution Robustness Evaluation of State-of-the-art Vision Models</h1> <p class="post-description">Research project comparing the robustness of 58 computer vision models</p> </header> <article> <h2 id="introduction">Introduction</h2> <p>This project provides a thorough out-of-distribution (OOD) robustness comparison across 58 state-of-the-art computer vision models. These include models based on vision transformers, convolutional networks, hybrid mechanisms, and more.</p> <p>The research investigates how different architectures perform under various distribution shifts, highlighting the strengths and weaknesses of each approach in terms of robustness.</p> <p>The full paper can be accessed on <a href="https://doi.org/10.48550/arXiv.2301.10750" rel="external nofollow noopener" target="_blank">arXiv</a> and the code is available on <a href="https://github.com/salman-lui/vision_course_project" rel="external nofollow noopener" target="_blank">GitHub</a>.</p> <p><span id="rahman2023out">Rahman, S., &amp; Lee, W. (2023). Out of distribution performance of state of art vision model. <i>ArXiv Preprint ArXiv:2301.10750</i>.</span></p> <h2 id="summary">Summary</h2> <p>This work offers a comparative study of out-of-distribution robustness across 58 models, such as vision transformers, CNNs, hybrid models, and others. The analysis was conducted using a unified training setup, covering diverse models like convolution-based, attention-based, sequence-based, and more. The findings reveal how robustness varies depending on model type and the OOD type being evaluated.</p> <h2 id="abstract">Abstract</h2> <p>Vision transformers (ViT) have emerged as cutting-edge tools in visual recognition tasks, often surpassing CNNs in robustness due to their self-attention mechanisms. However, previous studies may have been biased by unfair experimental setups, only comparing a small number of models.</p> <p>Our study explores 58 models, comparing attention, convolution, hybrid, and sequence-based architectures under fair and controlled conditions. Our findings demonstrate that robustness is not solely determined by architecture but also by training setup and OOD data type.</p> <p>The research aims to assist the community in better understanding the robustness of vision models.</p> <h2 id="methods">Methods</h2> <p>We evaluated these models using benchmark OOD datasets, such as ImageNet-A, ImageNet-R, ImageNet-Sketch, and others. We trained and tested models using standardized setups to ensure fair comparisons.</p> <ul> <li> <strong>Convolution-based models</strong>: ResNet-50, DenseNet, and EfficientNet</li> <li> <strong>Attention-based models</strong>: ViT, DeiT, BEiT</li> <li> <strong>Hybrid models</strong>: ConViT, ConvNext</li> <li> <strong>Other models</strong>: MLP-Mixer, RegNet, Sequencer</li> </ul> <h3 id="top-5-convolution-based-models">Top 5 Convolution-Based Models</h3> <table> <thead> <tr> <th><strong>Architecture</strong></th> <th><strong>ImageNet</strong></th> <th><strong>ImageNet -A</strong></th> <th><strong>ImageNet -O</strong></th> <th><strong>ImageNet -R</strong></th> <th><strong>Stylized-ImageNet</strong></th> <th><strong>ImageNet -Sketch</strong></th> </tr> </thead> <tbody> <tr> <td>ConvNext</td> <td>87.8</td> <td>35.79</td> <td>2.18</td> <td>51.71</td> <td>19.24</td> <td>38.22</td> </tr> <tr> <td>EfficientNetV2</td> <td>87.3</td> <td>22.31</td> <td>2.21</td> <td>52.97</td> <td>23.3</td> <td>40.33</td> </tr> <tr> <td>ResNeSt</td> <td>83.9</td> <td>5.47</td> <td>2.2</td> <td>37.77</td> <td>7.39</td> <td>25.28</td> </tr> <tr> <td>ResNet</td> <td>79.29</td> <td>1.88</td> <td>2.23</td> <td>28.93</td> <td>3.59</td> <td>16.84</td> </tr> <tr> <td>Xception</td> <td>79.88</td> <td>3.53</td> <td>2.28</td> <td>33.52</td> <td>9.33</td> <td>18.26</td> </tr> </tbody> </table> <h4 id="convolution-based-models">Convolution-Based Models</h4> <p>Convolution-based models, such as ConvNets, process visual data through layers of convolutions, pooling, and non-linear activations. They excel at capturing local spatial hierarchies in images, making them highly effective for tasks like object recognition and classification. Over time, innovations like depthwise separable convolutions and residual connections have increased their efficiency and depth, leading to architectures like <strong>ConvNext</strong> and <strong>EfficientNetV2</strong>.</p> <h3 id="top-5-attention-based-models">Top 5 Attention-Based Models</h3> <table> <thead> <tr> <th><strong>Architecture</strong></th> <th><strong>ImageNet</strong></th> <th><strong>ImageNet -A</strong></th> <th><strong>ImageNet -O</strong></th> <th><strong>ImageNet -R</strong></th> <th><strong>Stylized-ImageNet</strong></th> <th><strong>ImageNet -Sketch</strong></th> </tr> </thead> <tbody> <tr> <td>CaiT</td> <td>86.5</td> <td>34.4</td> <td>2.2</td> <td>49.95</td> <td>19.48</td> <td>36.03</td> </tr> <tr> <td>CrossViT</td> <td>81.5</td> <td>31.8</td> <td>2.2</td> <td>47.1</td> <td>18.51</td> <td>33.49</td> </tr> <tr> <td>DeiT3</td> <td>83.1</td> <td>40.12</td> <td>2.22</td> <td>53.63</td> <td>25.45</td> <td>40.06</td> </tr> <tr> <td>Swin</td> <td>87.3</td> <td>39.41</td> <td>2.19</td> <td>48.64</td> <td>18.36</td> <td>34.37</td> </tr> <tr> <td>VOLO</td> <td>87.1</td> <td>41.03</td> <td>2.23</td> <td>48.53</td> <td>17.74</td> <td>36.11</td> </tr> </tbody> </table> <h4 id="attention-based-models">Attention-Based Models</h4> <p>Attention-based models, like Vision Transformers (ViT), capture long-range dependencies within images by applying self-attention mechanisms. Instead of relying on convolutional operations, these models divide images into patches and use multi-head attention to learn relationships between different parts of the image. Attention models excel at capturing global context, achieving state-of-the-art results in tasks requiring holistic image understanding, as seen in models like <strong>Swin</strong> and <strong>DeiT3</strong>.</p> <h3 id="top-5-hybrid-convolution--attention-models">Top 5 Hybrid (Convolution + Attention) Models</h3> <table> <thead> <tr> <th><strong>Architecture</strong></th> <th><strong>ImageNet</strong></th> <th><strong>ImageNet -A</strong></th> <th><strong>ImageNet -O</strong></th> <th><strong>ImageNet -R</strong></th> <th><strong>Stylized-ImageNet</strong></th> <th><strong>ImageNet -Sketch</strong></th> </tr> </thead> <tbody> <tr> <td>ConViT</td> <td>81.3</td> <td>29.69</td> <td>2.21</td> <td>48.79</td> <td>19.81</td> <td>35.53</td> </tr> <tr> <td>EdgeNeXt</td> <td>71.2</td> <td>21.83</td> <td>2.23</td> <td>51.04</td> <td>16.26</td> <td>37.13</td> </tr> <tr> <td>LeViT</td> <td>80.0</td> <td>11.51</td> <td>2.22</td> <td>40.28</td> <td>15.59</td> <td>26.39</td> </tr> <tr> <td>RegNet</td> <td>80.25</td> <td>26.48</td> <td>2.17</td> <td>44.74</td> <td>11.88</td> <td>32.93</td> </tr> <tr> <td>Sequencer</td> <td>84.6</td> <td>35.36</td> <td>2.18</td> <td>48.56</td> <td>16.74</td> <td>35.87</td> </tr> </tbody> </table> <h4 id="hybrid-models-convolution--attention">Hybrid Models (Convolution + Attention)</h4> <p>Hybrid models combine the strengths of both convolution and attention mechanisms. These architectures use convolution layers to extract low-level features and attention mechanisms to capture global dependencies. By fusing these two approaches, hybrid models like <strong>ConViT</strong> and <strong>LeViT</strong> balance local feature extraction with global context understanding, achieving both efficiency and accuracy in visual tasks.</p> <h3 id="other-architectures">Other Architectures</h3> <table> <thead> <tr> <th><strong>Architecture</strong></th> <th><strong>ImageNet</strong></th> <th><strong>ImageNet -A</strong></th> <th><strong>ImageNet -O</strong></th> <th><strong>ImageNet -R</strong></th> <th><strong>Stylized-ImageNet</strong></th> <th><strong>ImageNet -Sketch</strong></th> </tr> </thead> <tbody> <tr> <td>MLPMixer</td> <td>76.44</td> <td>5.79</td> <td>2.25</td> <td>32.75</td> <td>10.04</td> <td>19.32</td> </tr> <tr> <td>MobileNetV3</td> <td>75.77</td> <td>4.59</td> <td>2.2</td> <td>35.11</td> <td>10.85</td> <td>22.95</td> </tr> <tr> <td>RegNet</td> <td>80.25</td> <td>26.48</td> <td>2.17</td> <td>44.74</td> <td>11.88</td> <td>32.93</td> </tr> <tr> <td>Sequencer</td> <td>84.6</td> <td>35.36</td> <td>2.18</td> <td>48.56</td> <td>16.74</td> <td>35.87</td> </tr> </tbody> </table> <h4 id="explanation-of-the-top-5-other-architectures">Explanation of the Top 5 Other Architectures</h4> <ol> <li> <p><strong>MLPMixer</strong>:</p> <ul> <li>Uses MLPs instead of convolutions or self-attention for vision tasks by mixing spatial and channel information. Efficient for large datasets.</li> </ul> </li> <li> <p><strong>MobileNetV3</strong>:</p> <ul> <li>Lightweight model designed for mobile devices, using depthwise separable convolutions and neural architecture search (NAS) to optimize performance with low computational cost.</li> </ul> </li> <li> <p><strong>RegNet</strong>:</p> <ul> <li>Scalable model family balancing depth and width, designed for efficient computation across different resource constraints, performing well on large vision tasks.</li> </ul> </li> <li> <p><strong>Sequencer</strong>:</p> <ul> <li>A vision model that adapts sequence modeling techniques from NLP, processing images as sequences to capture long-range dependencies.</li> </ul> </li> </ol> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="rahman2023out" class="col-sm-8"> <div class="title">Out of distribution performance of state of art vision model</div> <div class="author"> Salman Rahman, and <em>Wonkwon Lee</em> </div> <div class="periodical"> <em>arXiv preprint arXiv:2301.10750</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">rahman2023out</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Out of distribution performance of state of art vision model}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rahman, Salman and Lee, Wonkwon}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2301.10750}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Wonkwon Raymond Lee. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-48TQXH8T2C"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-48TQXH8T2C");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"publications",description:"publications in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"A collection of my projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-repositories",title:"repositories",description:"A brief showcases of my repositories",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"cv",description:"Professional Curriculum Vitae",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-mapping-the-mind-of-a-large-language-model-anthropic",title:'Mapping the Mind of a Large Language Model  Anthropic <svg width="1.2rem" height="1.2rem" top=".5rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"We have identified how millions of concepts are represented inside Claude Sonnet, one of our deployed large language models. This is the first ever detailed look inside a modern, production-grade large language model.",section:"Posts",handler:()=>{window.open("https://www.anthropic.com/research/mapping-mind-language-model","_blank")}},{id:"news-graduated-from-the-university-of-manchester-bsc-in-computer-science-and-mathematics",title:"Graduated from the University of Manchester, BSc in Computer Science and Mathematics!",description:"",section:"News"},{id:"news-graduated-from-new-york-university-courant-institute-of-mathematical-sciences-with-ms-in-computer-science-on-may-17-2023",title:"Graduated from New York University, Courant Institute of Mathematical Sciences with MS in...",description:"",section:"News"},{id:"projects-out-of-distribution-robustness-evaluation-of-state-of-the-art-vision-models",title:"Out-of-distribution Robustness Evaluation of State-of-the-art Vision Models",description:"Research project comparing the robustness of 58 computer vision models",section:"Projects",handler:()=>{window.location.href="/projects/1_ood/"}},{id:"projects-time-series-medical-image-classification",title:"Time-series Medical Image Classification",description:"Image Classification Model for Temporal Disease Progression of Chest X-ray dataset",section:"Projects",handler:()=>{window.location.href="/projects/2_timeseries_medical/"}},{id:"projects-epistemic-parity-reproducibility-as-an-evaluation-metric-for-differential-privacy",title:"Epistemic Parity: Reproducibility as an Evaluation Metric for Differential Privacy",description:"A benchmark and evaluation for reproducibility in differential privacy with state-of-the-art DP synthesizers.",section:"Projects",handler:()=>{window.location.href="/projects/3_synrd/"}},{id:"projects-nutritional-labels-for-automated-decision-systems-by-home-credit-default-risk",title:"Nutritional Labels for Automated Decision Systems by Home Credit Default Risk",description:"A fairness and explainability analysis on loan repayment predictions using machine learning models.",section:"Projects",handler:()=>{window.location.href="/projects/4_ads_nutrition_labels/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%77%6F%6E%6B%77%6F%6E.%6C%65%65@%6E%79%75.%65%64%75","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=xXZLYFwAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/wonkwonlee","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/wonkwon-lee","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>